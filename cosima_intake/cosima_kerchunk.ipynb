{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433c094c-3eb8-4328-8e70-a58d07da7e81",
   "metadata": {},
   "source": [
    "# Exploring building reference datasets with kerchunk\n",
    "\n",
    "The idea: build a test intake catalogue of kerchunk virtual dataset(s) from the COSIMA output(s) and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882bfa8d-6c53-4e60-af2c-5d46c6ae67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "\n",
    "import glob\n",
    "\n",
    "import fsspec\n",
    "\n",
    "import ujson\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78512d-c785-4e15-ad99-e8ba37977d24",
   "metadata": {},
   "source": [
    "# Build a reference dataset for COSIMA `access-om2-025` `025deg_jra55_iaf_omip2_cycle1` `ocean_month` data (~2.2TB, 61 netcdf files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8339c4d0-e36e-400e-97e2-613a1b133515",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_root = \"/g/data/ik11/outputs/access-om2-025/025deg_jra55_iaf_omip2_cycle1\"\n",
    "\n",
    "fs = fsspec.filesystem('file')\n",
    "flist = fs.glob(f\"{exp_root}/output*/ocean/ocean_month.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818ee69-ef16-41e1-8d07-933a03b09390",
   "metadata": {},
   "source": [
    "## Write single file jsons in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ebd1a8-4e81-4f96-9c9a-b9192b2248f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 13.7 s, total: 1min 32s\n",
      "Wall time: 13min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@dask.delayed\n",
    "def gen_json(file):\n",
    "    with fs.open(file) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, file)\n",
    "        outf = f\"{'.'.join(file.split('/')[5:])}.json\"\n",
    "        with open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode());\n",
    "            \n",
    "# This would take well over an hour without dask\n",
    "with Client(n_workers=8):\n",
    "    dask.compute(*[gen_json(file) for file in flist])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762759d-3936-44c2-820d-00393af7c8b1",
   "metadata": {},
   "source": [
    "## Combine into one multi-file json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c228e2d2-be1f-4a14-8852-e545ed8f1e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 50.4 s, total: 2min 47s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "json_list = fs.glob(\"./access-om2-025.025deg_jra55_iaf_omip2_cycle1.*.ocean.ocean_month.nc.json\")\n",
    "\n",
    "mzz = MultiZarrToZarr(\n",
    "    json_list,\n",
    "    concat_dims=['time'],\n",
    "    identical_dims=[\n",
    "        \"xt_ocean\", \n",
    "        \"yt_ocean\", \n",
    "        \"st_ocean\", \n",
    "        \"xu_ocean\", \n",
    "        \"yu_ocean\", \n",
    "        \"sw_ocean\", \n",
    "        \"grid_xt_ocean\", \n",
    "        \"grid_yt_ocean\", \n",
    "        \"grid_xu_ocean\", \n",
    "        \"grid_yu_ocean\", \n",
    "        \"potrho\", \n",
    "        \"neutral\",\n",
    "        \"nv\"\n",
    "     ],\n",
    ")\n",
    "\n",
    "d = mzz.translate(\"access-om2-025.025deg_jra55_iaf_omip2_cycle1.ocean_month.json\") # A 1.1 GB json!\n",
    "\n",
    "for json in json_list:\n",
    "    os.remove(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716c9af-a719-4e85-ac0b-a3a5a9976d71",
   "metadata": {},
   "source": [
    "## Load the reference dataset and do something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b248cf-702d-44ba-9e2c-ad0ca086eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard at: http://10.6.79.58:8787/status\n"
     ]
    }
   ],
   "source": [
    "client = Client(processes=False)\n",
    "print(f\"Dask dashboard at: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a76499-e7c1-4033-9d54-c4b44d086e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 2.77 s, total: 24.3 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m = fsspec.get_mapper(\n",
    "    'reference://', \n",
    "    fo=\"access-om2-025.025deg_jra55_iaf_omip2_cycle1.ocean_month.json\", \n",
    "    remote_protocol=\"file\"\n",
    ")\n",
    "ds = xr.open_dataset(\n",
    "    m,\n",
    "    engine='zarr', \n",
    "    backend_kwargs={\"consolidated\": False},\n",
    "    chunks={\"time\": -1},\n",
    "    decode_times=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330fcb1c-5a1a-425b-b699-c675beec2226",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.1 s, sys: 52.8 s, total: 1min 51s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This compute comprises 44 dask tasks and uses 4GB of unmanaged memory\n",
    "global_mean = ds[\"sst\"].mean([\"xt_ocean\", \"yt_ocean\"]).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf46dd7-b3c7-4f50-9338-3137cfcabdf1",
   "metadata": {},
   "source": [
    "## Compare to `open_mfdataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2eb79f4-3049-441e-96cc-e92b3a80a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 s, sys: 9.25 s, total: 30.5 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_mfdataset(\n",
    "    flist,\n",
    "    chunks={\"time\": -1},\n",
    "    concat_dim=\"time\",\n",
    "    parallel=True,\n",
    "    combine=\"nested\",\n",
    "    data_vars=\"minimal\", \n",
    "    coords=\"minimal\", \n",
    "    compat=\"override\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a8c2e4-140f-4227-b181-3e1c0473e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 3.33 s, total: 25.7 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This compute comprises 305 dask tasks and uses 2GB of unmanaged memory\n",
    "global_mean = ds[\"sst\"].mean([\"xt_ocean\", \"yt_ocean\"]).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b87749-bf15-48a8-9b40-37db6c566a33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Thoughts\n",
    "\n",
    "- The main impediment to this sort of approach at the moment is that kerchunk references are stored as inefficient json files which can be very large for datasets comprising many chunks. Currently this entire file is loaded with `open_dataset` (and loaded onto each worker when using a distributed cluster) which can require lots of memory and overhead. This outweighs the benefits kerchunk provides in this application (consolidation + zarr simplicity/performance).\n",
    "\n",
    "- This said, there are a number of things in the kerchunk pipeline that should address the above issue: 1) new data structures for the reference set(s) that facilitate lazy loading (see https://github.com/fsspec/kerchunk/issues/240 and https://github.com/fsspec/kerchunk/issues/134), 2) the ability the split/combine chunks within a reference file but this would have limitations (e.g. couldn't work for particular types of compression, see https://github.com/fsspec/kerchunk/issues/124 and https://github.com/fsspec/kerchunk/issues/134).\n",
    "\n",
    "- The approach of generating reference datasets for new experiments should be relatively straight-forward if we can have some confidence that all input files will be readily \"concatenatable\" in the sense that they have the same chunking patterns, coordinates on fixed dims, fixed auxiliary coordinates, variables (where approapriate) etc. The latest functionality for combining references in kerchunk is pretty versatile, see https://fsspec.github.io/kerchunk/tutorial.html#using-coo-map.\n",
    "\n",
    "- Where the raw data are unchunked (e.g. netcdf3), this approach can significantly impact performance since fsspec will make the full range request even for subsets (https://github.com/fsspec/kerchunk/issues/124)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c971fb3-17b7-4e30-bac9-6cc7c9244d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cosima-intake)",
   "language": "python",
   "name": "cosima-intake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
